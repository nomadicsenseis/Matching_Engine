{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "pCkebeBCsqvu",
        "outputId": "1528b159-e6e5-4b34-8fd2-acc5f74495d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.4.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.1.2 setuptools-62.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-uQ4b2Msz4v",
        "outputId": "1bee8a31-6c3f-4839-c90d-f9c2920d335d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.6/660.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.1/457.1 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting typing-extensions<4.2.0,>=3.7.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (62.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, langcodes, pydantic, pathy, catalogue, srsly, thinc, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.2.0\n",
            "    Uninstalling typing_extensions-4.2.0:\n",
            "      Successfully uninstalled typing_extensions-4.2.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.1 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.1 typing-extensions-4.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m41zx-YZs6F4",
        "outputId": "125e35bd-dfef-4db1-e2d7-f6fdd9e9d59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (62.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.3.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZfa4Dmgs-mu",
        "outputId": "3c59760a-92ce-4f0b-fb4d-6ea4ba8ded8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120735 sha256=f822c565f9b8f0005fdc74e6e192ce2186364ca95cb18b9fef40b72c22b5a78a\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, pyyaml, huggingface-hub, transformers, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD8zohWUtWrm",
        "outputId": "2520c150-8c0c-447b-8c94-473ecd1c2b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.21.6)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install rank_bm25 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIpAppQqtbsQ",
        "outputId": "b7001099-76de-4e08-c526-cc2cfd625584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting apache_beam\n",
            "  Downloading apache_beam-2.39.0-cp37-cp37m-manylinux2010_x86_64.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (4.1.1)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.1/508.1 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (6.0.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.7)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.20.6-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.17.3)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2022.1)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.46.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.21.6)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.6/256.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Collecting protobuf<4,>=3.12.2\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache_beam) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.24.3)\n",
            "Building wheels for collected packages: dill\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=896a7d5f89f0efa9ab5fd2cf7b488367f05f8f645c6c8f7130dfa1c8f2760e4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "Successfully built dill\n",
            "Installing collected packages: requests, pymongo, protobuf, orjson, fastavro, dill, cloudpickle, proto-plus, hdfs, apache_beam\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.1.1\n",
            "    Uninstalling pymongo-4.1.1:\n",
            "      Successfully uninstalled pymongo-4.1.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "multiprocess 0.70.13 requires dill>=0.3.5.1, but you have dill 0.3.1.1 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache_beam-2.39.0 cloudpickle-2.1.0 dill-0.3.1.1 fastavro-1.5.1 hdfs-2.7.0 orjson-3.7.2 proto-plus-1.20.6 protobuf-3.20.1 pymongo-3.12.3 requests-2.28.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install apache_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuZMYdWYt95D",
        "outputId": "ef353eba-d9cb-4fcc-94ae-fb70b51ca706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEZV8ldWy0nW",
        "outputId": "c1278377-ff58-4ce3-d2cf-6a0e495708c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
          ]
        }
      ],
      "source": [
        "#Versión de busqueda de atributos por BM25 y selección del más relevante por BERT. Falta matiza la elección del BRICK luego\n",
        "\n",
        "\n",
        "\n",
        "#To execute on a Jupiter lab the following instalations and imports are neeeded.\n",
        "'''pip install -U pip setuptools wheel\n",
        "pip install -U spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "pip install -U sentence-transformers\n",
        "pip install rank_bm25    \n",
        "pip install apache_beam'''\n",
        "    \n",
        "import io\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as et\n",
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import pickle\n",
        "import torch\n",
        "'''import cupy as cp'''\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "from rank_bm25 import BM25L\n",
        "import apache_beam as beam\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "    \n",
        "#Attributes\n",
        "#The GPC .xml database is imported with this notebook. \n",
        "#If missing, please refer to the GPC official webside and proceed to the free-download.\n",
        "attValues_df=pd.read_xml('GPC as of November 2021 v20211209 ES.xml',xpath=\"//brick/attType/attValue\")\n",
        "'''model='hackathon-pln-es/paraphrase-spanish-distilroberta' '''\n",
        "model='sentence-transformers/all-MiniLM-L6-v2' \n",
        "'''model='sentence-transformers/paraphrase-albert-small-v2' '''\n",
        "GPC_xml=et.parse('GPC as of November 2021 v20211209 ES.xml')\n",
        "attValue_embedding = pickle.load(open( \"serialattValueembedding.p\", \"rb\" ))\n",
        "rep_dict={'Spacy chunk':{'Query--attValue':'Brick'}}\n",
        "\n",
        "'''pickle.dump(rep_dict, open( \"rep_dict.p\", \"wb\" ))'''\n",
        "\n",
        "#The pickle file is imported with this notebook. In case is missing run the following:\n",
        "#attValue embedding\n",
        "'''attValue_embedding=embedder(model,attValues_df['text'].unique().tolist())'''\n",
        "#attValue_embedding picle format\n",
        "'''pickle.dump( attValue_embedding, open( \"serialattValueembedding.p\", \"wb\" ) )'''\n",
        "\n",
        "    \n",
        "def fromVal_getBricks(GPC_xml,attValue):\n",
        "    #Return grandparent of attValue (bricks)\n",
        "    bricks=[]\n",
        "    for brick in GPC_xml.findall(f'.//attValue[@text=\"{attValue}\"]/../..[@text]'):\n",
        "        tag='text'\n",
        "        attrib=brick.attrib\n",
        "        text=attrib[tag]\n",
        "        bricks.append(text)\n",
        "    return bricks\n",
        "    \n",
        "    \n",
        "def embedder(transformer_model,sentences):\n",
        "    model = SentenceTransformer(transformer_model, device='cuda')\n",
        "    embedding = model.encode(sentences)\n",
        "    return embedding\n",
        "    \n",
        "    \n",
        "'''def compare(corpus,query_embedding,corpus_embedding,metric,k):        \n",
        "    #Make the arrays in the device's catche using CuPy:\n",
        "    corpus_embedding_gpu=cp.asarray(corpus_embedding)\n",
        "    query_embedding_gpu=cp.asarray(query_embedding)\n",
        "\n",
        "    #Check that corpus and queries are on the same device\n",
        "    if corpus_embedding_gpu.device != query_embedding_gpu.device:\n",
        "        query_embedding_gpu = query_embedding_gpu.to(corpus_embedding_gpu.device)\n",
        "\n",
        "    #Use cupy's implementation of the scalar product wich optimizes the use of CUDA kernels:\n",
        "    scores=cp.dot(corpus_embedding_gpu,cp.transpose(query_embedding_gpu))\n",
        "\n",
        "            \n",
        "    #Take the top_k results in the GPU and then send them to the CPU\n",
        "    top_k=cp.asnumpy(cp.sort(cp.partition(scores, -k)[-k:])[::-1])\n",
        "    idx= cp.asnumpy(cp.argpartition(scores, -k)[-k:][cp.argsort(scores[cp.argpartition(scores, -k)[-k:]])][::-1])\n",
        "  \n",
        "   # Make a dictionary with every sentence that is compared to the query and its score. Return that dictionary.\n",
        "    top_res={}\n",
        "    for score, id in zip(top_k, idx):\n",
        "        top_res[corpus[id]]=score\n",
        "    return top_res'''\n",
        "\n",
        "\n",
        "def compare(corpus,query_embedding,corpus_embedding,metric,top_k,threshold):        \n",
        "    #Make sure a scalar product is used for comparison between the vectorized sentences.\n",
        "    if metric=='cos_sim':\n",
        "        scores=util.cos_sim(query_embedding,corpus_embedding)[0]\n",
        "            \n",
        "    #Take the top_k results\n",
        "    top_k = min(top_k, len(corpus_embedding))\n",
        "    top_results=torch.topk(scores,k=top_k)\n",
        "        \n",
        "    # Make a dictionary with every sentence that is compared to the query and its score. Return that dictionary.\n",
        "    top_res={}\n",
        "    for score, tensor in zip(top_results[0], top_results[1]):\n",
        "        idx=tensor.item()\n",
        "        if score.item()>threshold:\n",
        "          top_res[corpus[idx]]=score.item()\n",
        "    return top_res\n",
        "          \n",
        "#Como a BERT le paso la query como una lista (esto habría que tener MUCHO cuidado con ello) aqui altero un poco el código\n",
        "#pero solo funciona para listas con un imput...habria que llamarlo apra cada producto, no enviarle una lsita de productos\n",
        "def bm25(query,corpus,top_k):\n",
        "    '''query=[q.upper() for q in query]'''\n",
        "    query=query.upper()\n",
        "    tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
        "    bm25 = BM25L(tokenized_corpus)\n",
        "    \n",
        "    '''for q in query:\n",
        "        tokenized_query = q.split(\" \")\n",
        "        doc_scores = bm25.get_scores(tokenized_query)\n",
        "        results_query=bm25.get_top_n(tokenized_query, corpus, n=top_k)'''\n",
        "    \n",
        "    tokenized_query = query.split(\" \")\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    results_query=bm25.get_top_n(tokenized_query, corpus, n=top_k)\n",
        "    return results_query\n",
        "\n",
        "def spacy(query):\n",
        "    doc = nlp(query) \n",
        "    product=''\n",
        "    '''\n",
        "    count=0\n",
        "    for chunk in doc.noun_chunks:  \n",
        "        count=count+1\n",
        "        if count<3:\n",
        "            if product=='':\n",
        "                product=product+chunk.root.text\n",
        "                if chunk.root.text != chunk.root.head.text:\n",
        "                    product=product+' '+chunk.root.head.text\n",
        "            else: \n",
        "                if chunk.root.head.text in product:\n",
        "                    product=product+' '+chunk.root.text\n",
        "                else:\n",
        "                    product=product+' '+chunk.root.text\n",
        "                    if chunk.root.text != chunk.root.head.text:\n",
        "                        product=product+' '+chunk.root.head.text\n",
        "        else:\n",
        "            break\n",
        "    '''\n",
        "    if product=='':\n",
        "      product=doc[0].text\n",
        "    return product\n",
        "\n",
        "'''def actualice_dict(rep_dict):\n",
        "  pickle.dump(rep_dict, open( 'rep_dict.p', \"wb\" )) \n",
        "  return pickle.load(open('rep_dict.p', \"rb\" ))'''\n",
        "\n",
        "\n",
        "class Classifier(beam.DoFn):\n",
        "\n",
        "    def __init__(self,top_k,delimiter=','):\n",
        "        self.delimiter=delimiter\n",
        "        self.top_k=top_k\n",
        "        self.model=model\n",
        "        self.attValues_df=attValues_df\n",
        "        self.attValue_embedding=attValue_embedding\n",
        "        self.rep_dict=rep_dict\n",
        "\n",
        "    def process(self,text):\n",
        "        \n",
        "        #queries\n",
        "        for query in text.split(self.delimiter):\n",
        "            \n",
        "            #Clean query with spacy\n",
        "            spa_query=spacy(query)\n",
        "        \n",
        "            #Corpus of unique attributes\n",
        "            aVU=self.attValues_df['text'].unique().tolist()\n",
        "\n",
        "            #Check if spa_query has already been processed\n",
        "            top_brick={}\n",
        "            if spa_query in self.rep_dict.keys():\n",
        "              top_brick[query]=self.rep_dict[spa_query]\n",
        "            else:\n",
        "              self.rep_dict[spa_query]={}\n",
        "\n",
        "              #(spa)Query vectorization \n",
        "              spa_query_embedding=embedder(self.model,spa_query)\n",
        "\n",
        "              #Compare query with attValues using BERT based model\n",
        "              top_res_BERT=compare(aVU,spa_query_embedding,self.attValue_embedding,'cos_sim',self.top_k,0.5)\n",
        "        \n",
        "              #Compare query with attributes using tf-idf based model\n",
        "              top_res_bm25=bm25(spa_query,aVU,self.top_k)\n",
        "        \n",
        "              #Take only the attributes in the intersection if there are any, otherwise take the ones from BERT\n",
        "              top_res={}\n",
        "              for attValue,score in top_res_BERT.items(): \n",
        "                  if attValue in top_res_bm25:\n",
        "                      top_res[attValue]=score\n",
        "              if top_res:\n",
        "                  pass\n",
        "              else:\n",
        "                #check synonims or word embedding\n",
        "                top_res=top_res_BERT\n",
        "            \n",
        "              #Get bricks from attValues, vectorize them, and compare them with the query\n",
        "              if top_res:\n",
        "\n",
        "                '''#First architecture: score_attVal*score_brick\n",
        "                final_result={}\n",
        "                for attVal,score in top_res.items():\n",
        "                  top_bricks=fromVal_getBricks(GPC_xml,attVal)\n",
        "                  top_bricks_embedding=embedder(self.model,top_bricks)\n",
        "                  top_brick_res=compare(top_bricks,spa_query_embedding,top_bricks_embedding,'cos_sim',self.top_k,0)\n",
        "                                    \n",
        "                  for brick,br_score in top_brick_res.items():\n",
        "                      final_result[f'{query} --> {brick}:({attVal})']=score*br_score\n",
        "                top_brick=max(final_result, key=final_result.get)\n",
        "                self.rep_dict[spa_query]=top_brick'''\n",
        "                \n",
        "                #Second architecture: look for most common brick\n",
        "                top_rep_bricks={}\n",
        "                for attVal,score in top_res.items():\n",
        "                  top_bricks=fromVal_getBricks(GPC_xml,attVal)\n",
        "                  for brick in top_bricks:\n",
        "                    if brick in top_rep_bricks:\n",
        "                      top_rep_bricks[brick]=top_rep_bricks[brick]+score\n",
        "                    else:\n",
        "                      top_rep_bricks[brick]=score\n",
        "\n",
        "                top_rep_bricks_embedding=embedder(self.model,list(top_rep_bricks.keys()))\n",
        "                top_bricks_BERT=compare(list(top_rep_bricks.keys()),spa_query_embedding,top_rep_bricks_embedding,'cos_sim',self.top_k,0)\n",
        "                top_bricks={k: top_rep_bricks.get(k, 0) * top_bricks_BERT.get(k, 0) for k in set(top_rep_bricks) | set(top_bricks_BERT)}\n",
        "\n",
        "\n",
        "              \n",
        "\n",
        "                #Aquí falta saber cuando dos bricks tienen la misma puntuación\n",
        "                maxi=max(top_bricks, key=top_bricks.get)\n",
        "                top_brick={query:maxi}\n",
        "                self.rep_dict[spa_query]=maxi\n",
        "                  \n",
        "              \n",
        "              else:\n",
        "                top_brick[query]='No result found'\n",
        "                self.rep_dict[spa_query]='No result found'\n",
        "\n",
        "            top_brick_csv=f'{list(top_brick.keys())[0]},{list(top_brick.values())[0]}'\n",
        "\n",
        "            yield top_brick_csv\n",
        "                           \n",
        "\n",
        "          \n",
        "with beam.Pipeline() as pipeline:\n",
        "  products = (\n",
        "      pipeline\n",
        "      | \"Create df from csv\" >> beam.io.ReadFromText('consumomasivo.csv', skip_header_lines=True)\n",
        "      | 'map'>> beam.Map(lambda x: x.split(';')[2])\n",
        "      | 'Classifier' >> beam.ParDo(Classifier(20,','))\n",
        "      | 'Write to files' >> beam.io.WriteToText(\n",
        "          'productos_clasificados',\n",
        "          file_name_suffix='.txt'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Coperama_intersección_08062022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}